import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from peft import PeftModel  # <--- æ–°å¢å¼•å…¥
import logging
import gc
from typing import Optional, Dict, Any, List
from sentence_transformers import SentenceTransformer, util
import re
import os

# ç¡®ä¿ offload æ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs("offload_nllb", exist_ok=True)
logger = logging.getLogger(__name__)


class NeuralTranslator:
    def __init__(self, nmt_model_id="facebook/nllb-200-distilled-600M", 
                 reflection_model_id=None, 
                 lora_model_id=None,  # <--- æ–°å¢å‚æ•°ï¼šLoRA æ¨¡å‹è·¯å¾„
                 device='cpu'):
        
        # è‡ªåŠ¨æ£€æµ‹å¹¶è®¾ç½® GPU è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() and device == 'cuda' else 'cpu')
        self.nmt_tokenizer = None
        self.nmt_model = None
        self.reflector = None
        self.qe_model = None
        self.qe_threshold = 0.7
        
        # ä¿å­˜ LoRA è·¯å¾„
        self.lora_model_id = lora_model_id

        self.nmt_max_length = 150
        self.nmt_max_input_length = 128

        self._load_models(nmt_model_id, reflection_model_id)

    def _load_models(self, nmt_model_id: str, reflection_model_id: Optional[str]):
        try:
            # 1. åŠ è½½ NMT æ¨¡å‹ï¼ˆNLLBï¼Œæ”¯æŒå¤šè¯­è¨€ç¿»è¯‘ï¼‰
            logger.info(f"Loading NMT Base model: {nmt_model_id} (Device: {self.device})")
        
            # åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
            self.nmt_tokenizer = AutoTokenizer.from_pretrained(nmt_model_id)
            self.nmt_model = AutoModelForSeq2SeqLM.from_pretrained(
                nmt_model_id,
                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,
                load_in_8bit=False  # ä¿æŒ False
            ).to(self.device)

            # ğŸš€ æ–°å¢ LoRA æŒ‚è½½é€»è¾‘ ğŸš€
            if self.lora_model_id and os.path.exists(self.lora_model_id):
                logger.info(f"Loading LoRA Adapter from: {self.lora_model_id}")
            
                # åŠ è½½ LoRA æƒé‡
                lora_model = PeftModel.from_pretrained(
                    self.nmt_model, 
                    self.lora_model_id, 
                    adapter_name="nllb_lora" # å¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ªåç§°
                ).to(self.device)
            
                # åˆ‡æ¢åˆ° LoRA é€‚é…å™¨ï¼ˆå¯é€‰ï¼Œä½†é€šå¸¸éœ€è¦ï¼‰
                lora_model.set_adapter("nllb_lora") 
            
                # å°† PEFT æ¨¡å‹è®¾ç½®ä¸ºæ–°çš„ NMT æ¨¡å‹
                self.nmt_model = lora_model 
            
                logger.info("âœ… LoRA Adapter loaded and merged successfully. Model is ready for inference.")
        
            else:
                logger.info("âœ… NMT Base Model loaded successfully (No LoRA adapter found or used).")
                

            # 2. åŠ è½½åæ€æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
            if reflection_model_id:
                logger.info(f"Loading reflection model: {reflection_model_id}")
                
                self.reflector = pipeline(
                    "text-generation",
                    model=reflection_model_id,
                    dtype=torch.float32, # Reflection model usually keeps float32 or bfloat16
                    device_map="auto",
                    model_kwargs={
                        "low_cpu_mem_usage": True,
                        "use_safetensors": True
                    }
                )

                # ç¡®ä¿ reflector çš„ pad_token è®¾ç½®ç¨³å®š
                if self.reflector.tokenizer.pad_token is None and self.reflector.tokenizer.eos_token is not None:
                    self.reflector.tokenizer.pad_token = self.reflector.tokenizer.eos_token
                    self.reflector.model.config.pad_token_id = self.reflector.tokenizer.eos_token_id
                    logger.warning(f"Set pad_token/id to eos_token/id for reflection model.")

                logger.info("âœ… Reflection model loaded successfully.")
            else:
                logger.warning("No reflection model specified, skipping optimization.")

            # 3. åŠ è½½ QE æ¨¡å‹
            self._load_qe_model()

        except Exception as e:
            logger.error(f"Model load failed: {e}", exc_info=True)
            self._cleanup_vram()
            raise Exception(f"Translator init error: {str(e)}")

    def _load_qe_model(self):
        """åŠ è½½ç¿»è¯‘è´¨é‡è¯„ä¼°ï¼ˆQEï¼‰æ¨¡å‹"""
        try:
            self.qe_model = SentenceTransformer("all-MiniLM-L6-v2", device=self.device.type)
            logger.info("âœ… QE model (sentence-transformers) loaded successfully.")
        except Exception as e:
            logger.warning(f"QE model load failed: {e}", exc_info=True)
            self.qe_model = None

    def translate_segments(self, segments: List[Dict[str, Any]], target_lang: str, source_lang: str = 'auto',
                           use_reflection: bool = False, av_context: Optional[Dict[str, Any]] = None) -> List[
        Dict[str, Any]]:
        """
        ç¿»è¯‘å­—å¹•ç‰‡æ®µï¼ˆæ”¯æŒç‰‡æ®µçº§ AV ä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰
        """
        for idx, seg in enumerate(segments):
            if not all(key in seg for key in ["start", "end", "text"]):
                raise ValueError(f"Segment {idx + 1} missing required fields (start/end/text).")
            if "av_context" not in seg:
                seg["av_context"] = av_context or {}

        source_texts = [seg["text"].strip() for seg in segments]
        logger.info(
            f"Starting translation: {len(source_texts)} segments -> Target lang: {target_lang} (Reflection: {use_reflection})")

        # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡ç¿»è¯‘ï¼ˆåŸºç¡€ç¿»è¯‘ç»“æœï¼ŒåŒ…å« LoRA å½±å“ï¼‰
        translated_texts = self._translate_batch(source_texts, source_lang, target_lang)
        logger.info(f"Batch translation completed.")

        # ç¬¬äºŒæ­¥ï¼šåæ€ä¼˜åŒ–
        if use_reflection and self.reflector:
            logger.info("Starting reflection optimization with segment-level AV context...")
            optimized_texts = []

            for idx, (seg, src_text, trans_text) in enumerate(zip(segments, source_texts, translated_texts)):
                segment_av_ctx = seg["av_context"] or av_context or {}
                optimized = self._reflect_and_improve(src_text, trans_text, target_lang, segment_av_ctx, idx + 1)
                optimized_texts.append(optimized)
            translated_texts = optimized_texts
            logger.info("Reflection optimization completed.")

        # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®— QE åˆ†æ•°
        qe_scores = self._calculate_batch_qe_scores(source_texts, translated_texts) if self.qe_model else [0.0] * len(
            source_texts)

        # ç¬¬å››æ­¥ï¼šç»„è£…æœ€ç»ˆç»“æœ
        result = []
        for idx, (seg, trans_text, qe_score) in enumerate(zip(segments, translated_texts, qe_scores)):
            result.append({
                "start": round(seg["start"], 2),
                "end": round(seg["end"], 2),
                "text": trans_text,
                "original_text": seg["text"],
                "qe_score": round(qe_score, 2),
                "av_context": seg["av_context"],
                "is_optimized": use_reflection and self.reflector is not None
            })

        logger.info(f"Translation process finished: {len(result)} segments processed.")

        return result

    def _translate_batch(self, texts: List[str], src_lang_code: str, tgt_lang_code: str) -> List[str]:
        """
        æ‰¹é‡ç¿»è¯‘æ–‡æœ¬ï¼ˆNLLB æ¨¡å‹æ ¸å¿ƒç¿»è¯‘é€»è¾‘ï¼‰
        """
        lang_map = {
            'auto': 'eng_Latn', 'en': 'eng_Latn', 'zh': 'zho_Hans', 'zh-cn': 'zho_Hans',
            'ja': 'jpn_Jpan', 'ko': 'kor_Hang', 'fr': 'fra_Latn', 'de': 'deu_Latn',
            'es': 'spa_Latn', 'ru': 'rus_Cyrl', 'ar': 'ara_Arab', 'hi': 'hin_Deva',
            'pt': 'por_Latn', 'it': 'ita_Latn', 'nl': 'nld_Latn', 'pl': 'pol_Latn'
        }

        src_code = lang_map.get(src_lang_code.lower(), 'eng_Latn')
        tgt_code = lang_map.get(tgt_lang_code.lower(), 'zho_Hans')
        
        inputs = self.nmt_tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.nmt_max_input_length
        ).to(self.device)

        forced_bos_token_id = self.nmt_tokenizer.convert_tokens_to_ids(tgt_code)

        with torch.no_grad():
            generated_tokens = self.nmt_model.generate(
                **inputs,
                forced_bos_token_id=forced_bos_token_id,
                max_length=self.nmt_max_length,
                num_beams=4,
                do_sample=False,
                early_stopping=True,
                no_repeat_ngram_size=2
            )

        translations = self.nmt_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        translations = [trans.strip() for trans in translations]

        return translations

    def _reflect_and_improve(self, source_text: str, initial_translation: str, tgt_lang: str,
                             segment_av_ctx: Dict[str, Any], segment_idx: int) -> str:
        """
        ç»“åˆç‰‡æ®µçº§ AV ä¸Šä¸‹æ–‡ä¼˜åŒ–ç¿»è¯‘ç»“æœï¼ˆåæ€æœºåˆ¶ï¼‰
        """
        scene_type = segment_av_ctx.get("scene_type", "æ— /æœªçŸ¥").strip()
        environment = segment_av_ctx.get("environment", "æ— /æœªçŸ¥").strip()
        emotion = segment_av_ctx.get("emotion", "æ— /æœªçŸ¥").strip()
        activity = segment_av_ctx.get("activity", "æ— /æœªçŸ¥").strip()
        scene_desc = segment_av_ctx.get("description", "æ— è¯¦ç»†æè¿°").strip()

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ã€åœºæ™¯æ„ŸçŸ¥çš„å­—å¹•ç¿»è¯‘åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„**æ‰€æœ‰**åœºæ™¯ä¿¡æ¯å’Œè§†è§‰æè¿°ï¼Œä¼˜åŒ–ç»™å®šçš„ç¿»è¯‘ç»“æœï¼Œä»¥ç¡®ä¿ç¿»è¯‘çš„è¯æ±‡ã€é£æ ¼å’Œæƒ…æ„Ÿä¸åœºæ™¯é«˜åº¦åŒ¹é…ã€‚

=== å½“å‰åœºæ™¯ä¿¡æ¯ (æ¥è‡ª VLM çš„æå–ç»“æœ) ===
- åœºæ™¯ç±»å‹: {scene_type}
- å…·ä½“ç¯å¢ƒ: {environment}
- äººç‰©æƒ…æ„Ÿ: {emotion}
- æ´»åŠ¨çŠ¶æ€: {activity}
- è¯¦ç»†è§†è§‰æè¿°: {scene_desc}

=== ç¿»è¯‘å®ˆåˆ™ ===
1. ã€å‡†ç¡®æ€§ã€‘ç¿»è¯‘å¿…é¡»ä¸¥æ ¼å¿ äºåŸæ–‡å«ä¹‰ï¼Œä¸å¾—å¢åˆ è¯­ä¹‰ã€‚
2. ã€åœºæ™¯é€‚é…ã€‘è¯·æ ¹æ®**ä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯**ï¼Œé€‰æ‹©æœ€ç¬¦åˆåœºæ™¯ï¼ˆä¾‹å¦‚ï¼šæ¸¸æˆã€ç›´æ’­ã€åŒ»ç–—ã€æ³•å¾‹ã€æ—¥å¸¸ç”Ÿæ´»ç­‰ï¼‰çš„ä¸“ä¸šæœ¯è¯­å’Œå£è¯­åŒ–é£æ ¼ã€‚
3. ã€æƒ…æ„ŸåŒ¹é…ã€‘ç¿»è¯‘ç»“æœåº”èƒ½åæ˜ äººç‰©çš„æƒ…æ„ŸçŠ¶æ€ï¼ˆä¾‹å¦‚ï¼šå…´å¥‹ã€å¹³é™ã€ä¸¥è‚ƒï¼‰ã€‚
4. ã€ç®€æ´æ€§ã€‘å­—å¹•éœ€ç®€çŸ­ç²¾ç‚¼ï¼Œæ˜“äºè§‚ä¼—å¿«é€Ÿé˜…è¯»ã€‚
5. ã€ç›®æ ‡è¯­è¨€ã€‘è¯·ç¿»è¯‘æˆ{self._get_lang_name(tgt_lang)}ã€‚

=== éœ€è¦ä¼˜åŒ–çš„å†…å®¹ ===
- æºæ–‡æœ¬: "{source_text}"
- åŸºç¡€ç¿»è¯‘: "{initial_translation}"

è¯·æ ¹æ®ä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ï¼Œè¾“å‡ºä¼˜åŒ–åçš„æœ€ç»ˆç¿»è¯‘ç»“æœã€‚
**ğŸ”´ æ ¸å¿ƒæŒ‡ä»¤: ä½ çš„å›ç­”ä¸­ï¼Œå¿…é¡»ä¸”åªèƒ½åŒ…å«æœ€ç»ˆä¼˜åŒ–åçš„çº¯å‡€ä¸­æ–‡å­—å¹•æ–‡æœ¬ï¼Œä¸å…è®¸åŒ…å«ä»»ä½•è§£é‡Šã€åˆ†æã€æ‰“æ‹›å‘¼æˆ–é¢å¤–çš„æ–‡å­—ã€‚è¯·ç«‹å³å¼€å§‹è¾“å‡ºç¿»è¯‘æ–‡æœ¬ã€‚**
""" 

        try:
            prompt_text = prompt.strip()
            pad_id = self.reflector.model.config.pad_token_id
            eos_id = self.reflector.model.config.eos_token_id

            response = self.reflector(
                prompt_text,
                max_new_tokens=150,
                do_sample=False,
                num_return_sequences=1,
                pad_token_id=pad_id,
                eos_token_id=eos_id,
            )[0]["generated_text"]

            optimized = initial_translation 

            if response.startswith(prompt_text):
                optimized = response[len(prompt_text):].strip()
            else:
                optimized = response.strip()

            optimized = re.sub(r'(Human:|Assistant:|\n\n).*', '', optimized, flags=re.IGNORECASE | re.DOTALL).strip()
            optimized = optimized.replace("<|endoftext|>", "").strip()
            optimized = optimized.strip('"').strip("'").strip()
            optimized = optimized.replace('\n', ' ').strip()

            if not optimized or len(optimized) < 1:
                return initial_translation

            return optimized

        except Exception as e:
            logger.error(f"Segment {segment_idx} reflection failed: {e}", exc_info=True)
            return initial_translation

    def _calculate_batch_qe_scores(self, source_texts: List[str], translated_texts: List[str]) -> List[float]:
        if not source_texts or not translated_texts or len(source_texts) != len(translated_texts):
            return [0.0] * len(source_texts)

        try:
            src_embeddings = self.qe_model.encode(source_texts, convert_to_tensor=True, show_progress_bar=False)
            trans_embeddings = self.qe_model.encode(translated_texts, convert_to_tensor=True, show_progress_bar=False)
            similarities = util.cos_sim(src_embeddings, trans_embeddings).diag().cpu().numpy()
            qe_scores = [float(max(0, sim)) for sim in similarities]
            return qe_scores
        except Exception as e:
            logger.error(f"QE score calculation failed: {e}", exc_info=True)
            return [0.0] * len(source_texts)

    def _get_lang_name(self, lang_code: str) -> str:
        lang_names = {
            'zh': 'ä¸­æ–‡', 'zh-cn': 'ä¸­æ–‡', 'en': 'è‹±æ–‡', 'ja': 'æ—¥æ–‡', 'ko': 'éŸ©æ–‡',
            'fr': 'æ³•æ–‡', 'de': 'å¾·æ–‡', 'es': 'è¥¿ç­ç‰™æ–‡', 'ru': 'ä¿„æ–‡', 'ar': 'é˜¿æ‹‰ä¼¯æ–‡',
            'pt': 'è‘¡è„ç‰™æ–‡', 'it': 'æ„å¤§åˆ©æ–‡', 'nl': 'è·å…°æ–‡', 'pl': 'æ³¢å…°æ–‡'
        }
        return lang_names.get(lang_code.lower(), lang_code)

    def _cleanup_vram(self, nmt_only: bool = False):
        if self.nmt_model:
            del self.nmt_model
            self.nmt_model = None

        if not nmt_only:
            if self.reflector:
                del self.reflector
                self.reflector = None
            if self.qe_model:
                del self.qe_model
                self.qe_model = None

        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def get_supported_languages(self) -> Dict[str, List[str]]:
        return {
            "whisper": ["auto", "en", "zh", "ja", "ko", "fr", "de", "es", "ru", "ar", "hi", "pt", "it", "nl", "pl"],
            "nmt": ["en", "zh", "zh-cn", "ja", "ko", "fr", "de", "es", "ru", "ar", "hi", "pt", "it", "nl", "pl"]
        }

    def __del__(self):
        self._cleanup_vram()