# -*- coding: utf-8 -*-
"""
NLLB-200 LoRA å¾®è°ƒè®­ç»ƒè„šæœ¬
ä¸“æ³¨äºä¸“ä¸šè¯æ±‡å’Œå¤šä¹‰è¯çš„ç²¾å‡†ç¿»è¯‘
"""

import os
import json
import torch
import logging
from dataclasses import dataclass, field
from typing import Optional, List, Dict
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from datasets import Dataset
import evaluate

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class LoraFineTuneConfig:
    """LoRA å¾®è°ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    base_model: str = "facebook/nllb-200-distilled-600M"
    lora_output_dir: str = "models/lora_checkpoints"
    
    # LoRA å‚æ•°ï¼ˆæ ¸å¿ƒï¼‰
    lora_r: int = 16  # LoRA ç§©ï¼ˆè¶Šå¤§æ•ˆæœè¶Šå¥½ï¼Œä½†è®­ç»ƒè¶Šæ…¢ï¼‰
    lora_alpha: int = 32  # LoRA ç¼©æ”¾ç³»æ•°ï¼ˆé€šå¸¸æ˜¯ r çš„ 2 å€ï¼‰
    lora_dropout: float = 0.1
    lora_target_modules: List[str] = field(default_factory=lambda: [
        "q_proj",  # Query æŠ•å½±å±‚
        "v_proj",  # Value æŠ•å½±å±‚
        "k_proj",  # Key æŠ•å½±å±‚
        "out_proj",  # è¾“å‡ºæŠ•å½±å±‚
        "fc1",  # FFN ç¬¬ä¸€å±‚
        "fc2",  # FFN ç¬¬äºŒå±‚
    ])
    
    # è®­ç»ƒè¶…å‚æ•°
    num_epochs: int = 10
    learning_rate: float = 3e-4
    batch_size: int = 8
    gradient_accumulation_steps: int = 4
    warmup_steps: int = 100
    max_grad_norm: float = 1.0
    weight_decay: float = 0.01
    
    # æ—©åœç­–ç•¥
    early_stopping_patience: int = 3
    early_stopping_threshold: float = 0.001
    
    # æ•°æ®è·¯å¾„
    train_data_path: str = "data/finetune/train/data.json"
    eval_data_path: str = "data/finetune/eval/data.json"
    
    # è®¾å¤‡é…ç½®
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    fp16: bool = torch.cuda.is_available()


class NLLBLoraTrainer:
    """NLLB LoRA å¾®è°ƒè®­ç»ƒå™¨"""
    
    def __init__(self, config: LoraFineTuneConfig):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.bleu_metric = evaluate.load("sacrebleu")
        
    def load_base_model(self):
        """åŠ è½½åŸºç¡€ NLLB æ¨¡å‹"""
        logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {self.config.base_model}")
        
        # åŠ è½½ tokenizer å¹¶è®¾ç½®æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.base_model,
            src_lang="eng_Latn",  # è‹±è¯­ä½œä¸ºæºè¯­è¨€
            tgt_lang="zho_Hans"   # ä¸­æ–‡ä½œä¸ºç›®æ ‡è¯­è¨€
        )
        
        # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ 8-bit é‡åŒ–èŠ‚çœæ˜¾å­˜ï¼‰
        self.model = AutoModelForSeq2SeqLM.from_pretrained(
            self.config.base_model,
            load_in_8bit=True if self.config.device == "cuda" else False,
            device_map="auto" if self.config.device == "cuda" else None,
            torch_dtype=torch.float16 if self.config.fp16 else torch.float32
        )
        
        logger.info("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def apply_lora(self):
        """åº”ç”¨ LoRA é€‚é…å™¨"""
        logger.info("åº”ç”¨ LoRA é…ç½®...")
        
        lora_config = LoraConfig(
            task_type=TaskType.SEQ_2_SEQ_LM,
            inference_mode=False,
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            lora_dropout=self.config.lora_dropout,
            target_modules=self.config.lora_target_modules,
            bias="none"
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        logger.info("âœ… LoRA é…ç½®åº”ç”¨å®Œæˆ")
    
    def load_datasets(self) -> Dict[str, Dataset]:
        """åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        logger.info("åŠ è½½æ•°æ®é›†...")
        
        # åŠ è½½ JSON æ•°æ®
        with open(self.config.train_data_path, 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        
        with open(self.config.eval_data_path, 'r', encoding='utf-8') as f:
            eval_data = json.load(f)
        
        # è½¬æ¢ä¸º Hugging Face Dataset
        train_dataset = Dataset.from_list(train_data)
        eval_dataset = Dataset.from_list(eval_data)
        
        # Tokenize
        def preprocess_function(examples):
            # æºè¯­è¨€ç¼–ç 
            inputs = self.tokenizer(
                examples["src"],
                max_length=88,
                truncation=True,
                padding="max_length"
            )
            
            # ç›®æ ‡è¯­è¨€ç¼–ç ï¼ˆä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ tokenizerï¼Œä¸ä½¿ç”¨ as_target_tokenizerï¼‰
            labels = self.tokenizer(
                examples["tgt"],
                max_length=88,
                truncation=True,
                padding="max_length"
            )
            
            inputs["labels"] = labels["input_ids"]
            return inputs
        
        train_dataset = train_dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=train_dataset.column_names
        )
        
        eval_dataset = eval_dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=eval_dataset.column_names
        )
        
        logger.info(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: Train={len(train_dataset)}, Eval={len(eval_dataset)}")
        
        return {"train": train_dataset, "eval": eval_dataset}
    
    def compute_metrics(self, eval_preds):
        """è®¡ç®— BLEU åˆ†æ•°"""
        preds, labels = eval_preds
        
        # è§£ç é¢„æµ‹
        if isinstance(preds, tuple):
            preds = preds[0]
        
        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)
        
        # å¤„ç†æ ‡ç­¾ï¼ˆæ›¿æ¢ -100 ä¸º pad_token_idï¼‰
        labels = [[l if l != -100 else self.tokenizer.pad_token_id for l in label] for label in labels]
        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
        
        # è®¡ç®— BLEU
        result = self.bleu_metric.compute(
            predictions=decoded_preds,
            references=[[label] for label in decoded_labels]
        )
        
        return {"bleu": result["score"]}
    
    def train(self):
        """æ‰§è¡Œå¾®è°ƒè®­ç»ƒ"""
        # 1. åŠ è½½æ¨¡å‹å’Œæ•°æ®
        self.load_base_model()
        self.apply_lora()
        datasets = self.load_datasets()
        
        # 2. è®­ç»ƒå‚æ•°
        training_args = Seq2SeqTrainingArguments(
            output_dir=self.config.lora_output_dir,
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            warmup_steps=self.config.warmup_steps,
            max_grad_norm=self.config.max_grad_norm,
            fp16=self.config.fp16,
            
            # è¯„ä¼°ç­–ç•¥
            eval_strategy="steps",
            eval_steps=750,
            save_strategy="steps",
            save_steps=750,
            save_total_limit=2,
            load_best_model_at_end=True,
            metric_for_best_model="bleu",
            greater_is_better=True,
            
            # æ—¥å¿—
            logging_steps=300,
            logging_dir=f"{self.config.lora_output_dir}/logs",
            report_to="none",
            
            # ç”Ÿæˆé…ç½®
            predict_with_generate=True,
            generation_max_length=88,
            generation_num_beams=4,
        )
        
        # 3. æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForSeq2Seq(
            self.tokenizer,
            model=self.model,
            padding=True
        )
        
        # 4. æ—©åœå›è°ƒ
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=self.config.early_stopping_patience,
            early_stopping_threshold=self.config.early_stopping_threshold
        )
        
        # 5. åˆå§‹åŒ– Trainer
        trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            train_dataset=datasets["train"],
            eval_dataset=datasets["eval"],
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
            callbacks=[early_stopping]
        )
        
        # 6. å¼€å§‹è®­ç»ƒ
        logger.info("ğŸš€ å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ...")
        train_result = trainer.train()
        
        # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
        logger.info("ä¿å­˜å¾®è°ƒåçš„ LoRA æ¨¡å‹...")
        trainer.save_model()
        self.tokenizer.save_pretrained(self.config.lora_output_dir)
        
        # 8. ä¿å­˜è®­ç»ƒæŒ‡æ ‡
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        
        logger.info("âœ… LoRA å¾®è°ƒè®­ç»ƒå®Œæˆï¼")
        logger.info(f"   æ¨¡å‹ä¿å­˜è·¯å¾„: {self.config.lora_output_dir}")
        
        return metrics


# ============================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================
if __name__ == "__main__":
    # é…ç½®
    config = LoraFineTuneConfig(
        base_model="facebook/nllb-200-1.3B",
        lora_output_dir="models/lora_nllb_terminology",
        num_epochs=8,
        batch_size=8,  # å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œé™ä½åˆ° 2
        gradient_accumulation_steps=2,
        learning_rate=2e-4,
        lora_r=16,
        lora_alpha=32,
        warmup_steps=50,  # çƒ­èº«æ­¥æ•°å‡å°‘
    )
    
    # è®­ç»ƒ
    trainer = NLLBLoraTrainer(config)
    metrics = trainer.train()
    
    print("\n" + "="*50)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæŒ‡æ ‡:")
    print(f"   Loss: {metrics.get('train_loss', 'N/A'):.4f}")
    print("="*50)